# The Spider Farm

![alt text](/spider_farm1.jpg "A rancher tends to his webcrawlers.")

## Table of Contents

- [Description](#description)
- [Features](#features)
- [Usage](#usage)
- [License](#license)
- [Contact](#contact)

## Description

A collection of spiders built on the [Scrapy](https://scrapy.org) framework, designed for targeted link discovery, schema extraction, and dynamic content crawling using **Playwright**.

---

## Features

![Python](https://img.shields.io/badge/Python-3.11%2B-blue.svg)
![Scrapy](https://img.shields.io/badge/Built%20With-Scrapy-brightgreen.svg)
![Playwright](https://img.shields.io/badge/JS%20Rendering-Playwright-ff69b4.svg)
![CLI Tool](https://img.shields.io/badge/CLI-Enabled-orange.svg)

---

### **Playwright Integration**
- Handles **JavaScript-rendered pages** directly inside Scrapy.
- Supports custom **navigation waits** (`domcontentloaded`, `networkidle`, etc.).
- Passes **custom headers** (e.g., Google referer spoofing) to bypass basic bot protection.
- Works seamlessly with all spiders via `meta` configuration.
- Uses Scrapy’s `DOWNLOAD_HANDLERS` for Playwright — no manual middleware configuration needed.
- Reference the below links for installing and setting **Playwright for Python** up for use in **Scrapy**:
  - [Microsoft Playwright](https://github.com/microsoft/playwright)
  - [Playwright for Python](https://playwright.dev/python/docs/intro)
  - [Scrapy-Playwright](https://github.com/scrapy-plugins/scrapy-playwright)

---

### **XMLSpider**
A sitemap-focused crawler for extracting and verifying URLs from XML sitemaps.

* **Namespace-aware and namespace-agnostic parsing** to work with sitemaps that use XML namespaces (including `http` or `https` variations) or no namespace at all.
* **Direct sitemap URL extraction** from `<loc>` elements inside `<url>` nodes, without unnecessary page content parsing.
* **HTTP status verification** requests each URL from the sitemap and records its HTTP status code.
* **Duplicate-awareness** to avoid processing the same URL more than once.
* **Large sitemap progress logging**  featuring INFO-level log every 250 URLs processed for monitoring crawl progress.
* **Compatible with standard XML sitemap format** generated by most CMS and e-commerce platforms.

---

### **LinkSpider**
A configurable web crawler for link discovery and cataloging.

* **Recursive crawling option** with custom or infinite crawl depth.
* **Targeted link discovery** by specifying HTML tag and attribute (e.g., `a[href]`, `link[href]`).
* **Scoped content extraction** using container tags like `div.article` or `div#main`.
* **Include / Exclude filtering** for URLs (e.g., include `nike`, exclude `sale`).
* **Metadata extraction** from each page (`title`, `meta description`, `canonical link`).
* **Non-HTML resource filtering** to skip images, PDFs, scripts, etc.
* **Duplicate-awareness** to avoid reprocessing the same links.
* **Domain-restricted crawling** to the seed domain and its subdomains.
* **Auto-save or view modes** allowing save as CSV or inspect directly in terminal.
* **CLI interface** for automation, logging control, and filename customization.

---

### **SchemaSpider**
A considerate crawler that discovers and extracts Schema.org structured data from web pages, including JavaScript-rendered content.
Everything in **LinkSpider**, plus:

* **Automatic Schema.org structured data extraction** supporting `JSON-LD`, `Microdata`, and `RDFa` and normalizes schema output for analysis.
* **Playwright-powered schema crawling** that loads JavaScript-heavy pages to reveal client-side schema markup.
* **Recursive schema discovery** to optionally follow links and extracts schema data across multiple pages.

---

## **Usage - Basic Example**

### *XMLSpider*
```bash
python main.py --spider xml \
               --url "https://exmaple.com/sitemap.xml" \
               --auto save \
               --output my_sitemap \
               --log info
```

### *LinkSpider*
```bash
python main.py --spider link \
               --url "https://example.com" \
               --tag a \
               --attr href \
               --ctag div#main \
               --depth 3 \
               --log INFO \
               --auto save \
               --output my_links \
               --include nike,adidas \
               --exclude sale,outlet
```

### *SchemaSpider with Playwright*
```bash
python main.py --spider schema \
               --url "https://example.com/product/123" \
               --crawl \
               --depth 2 \
               --auto view \
               --log debug
```

---

### Available CLI Options

| Option      | Description                                                            | Default      |
| ----------- | ---------------------------------------------------------------------- | ------------ |
| `--spider`  | Choose spider: `link` (default) or `schema`                            | `link`       |
| `--crawl`   | Enable recursive crawling (honors depth)                               | *(optional)* |
| `--url`     | Starting URL (wrap in quotes)                                          | *(required)* |
| `--tag`     | HTML tag to search for (`a`, `link`, etc.)                             | `a`          |
| `--attr`    | Attribute from which to extract links                                  | `href`       |
| `--ctag`    | Optional container tag selector (`div.article`, `div#main`)            | *(optional)* |
| `--depth`   | Maximum crawl depth (`0` = infinite)                                   | `2`          |
| `--log`     | Log verbosity: `NONE`, `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL` | `INFO`       |
| `--auto`    | Auto output mode: `save` (CSV), `view` (table), or omit to be prompted | *(optional)* |
| `--output`  | Custom output filename (no extension)                                  | *(optional)* |
| `--include` | Comma-separated values that must appear in URL                         | *(optional)* |
| `--exclude` | Comma-separated values to exclude from URL                             | *(optional)* |

---

## Licenses

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
This project is licensed under the [MIT License](LICENSE).

### Third-Party Licenses

This project uses:

* [Scrapy](https://scrapy.org) — ![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)
* [Playwright, for Python](https://github.com/microsoft/playwright-python) - ![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)
* [Scrapy-Playwright](https://github.com/scrapy-plugins/scrapy-playwright) — ![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)(BSD 3-Clause License)

See [THIRD_PARTY_LICENSES](THIRD_PARTY_LICENSES.md) for full text and license source links.

---

## Contact

* Joe Thompson (@jopeymonster)
* [https://github.com/jopeymonster](https://github.com/jopeymonster)
